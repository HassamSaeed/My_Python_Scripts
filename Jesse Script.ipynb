{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GSpread\n",
    "\n",
    "from datetime import datetime\n",
    "startTime = datetime.now()\n",
    "\n",
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "scope = ['https://spreadsheets.google.com/feeds',\n",
    "         'https://www.googleapis.com/auth/drive']\n",
    "credentials = ServiceAccountCredentials.from_json_keyfile_name('ardent-sun-269107-89f85b1ebcee.json', scope)\n",
    "gc = gspread.authorize(credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get URLs to be parsed from Google Sheet\n",
    "sh = gc.open_by_key('1epSSuflRy8tYmaETIPZj0CfRo9NnzyK4t5ApT7M6p7A')\n",
    "work_sheet = sh.worksheet(\"Sheet2\")\n",
    "\n",
    "# Get URLs to be parsed from Google Sheet\n",
    "sh2 = gc.open_by_key('1epSSuflRy8tYmaETIPZj0CfRo9NnzyK4t5ApT7M6p7A')\n",
    "work_sheet2 = sh.worksheet(\"Sheet3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = work_sheet.get_all_values()\n",
    "headers = data.pop(0)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2= [item for item in work_sheet2.col_values(1) if item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(data, columns=headers)\n",
    "unique_data = df.drop_duplicates(subset = 'website_url')\n",
    "\n",
    "df = df[~df['website_url'].isin(data2)]\n",
    "\n",
    "header_row = [['business_name','address','address_city','address_state','address_zip','phone#','website_url']]\n",
    "unique_data_lol = header_row + unique_data.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_sheet.clear()\n",
    "\n",
    "# #Update 'Bloomingdales' sheet with parsed data\n",
    "import numpy as np\n",
    "cells = []\n",
    "\n",
    "for row_num, row in enumerate(unique_data_lol):\n",
    "    for col_num, cell in enumerate(row):\n",
    "        cells.append(gspread.Cell(row_num + 2, col_num + 1, unique_data_lol[row_num][col_num]))\n",
    "\n",
    "work_sheet.update_cells(cells)\n",
    "\n",
    "print('Woohooo!!! Your Big Data is successfully updated in Google Sheets. Please check out your Google Sheet.')\n",
    "print('Total Execution Time:'+ str(datetime.now() - startTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get URLs to be parsed from Google Sheet\n",
    "sh = gc.open_by_key('1epSSuflRy8tYmaETIPZj0CfRo9NnzyK4t5ApT7M6p7A')\n",
    "work_sheet = sh.worksheet(\"Sheet4\")\n",
    "\n",
    "new_worksheet = sh.add_worksheet(title=\"Sheet6\", rows=\"100\", cols=\"20\")\n",
    "\n",
    "sh.del_worksheet(work_sheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0   1   2\n",
      "0   1   2   3\n",
      "1   4   5   6\n",
      "2   7   8   9\n",
      "3  11  12  13\n",
      "4   1   2   3\n",
      "    0   1   2\n",
      "5   4   5   6\n",
      "6   7   8   9\n",
      "7  11  12  13\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "a = [[1,2,3],[4,5,6],[7,8,9],[11,12,13],[1,2,3],[4,5,6],[7,8,9],[11,12,13]]\n",
    "pdf = pd.DataFrame(a)\n",
    "\n",
    "for i, df in pdf.groupby(np.arange(len(pdf)) // 5):\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625617\n"
     ]
    }
   ],
   "source": [
    "chunk = -(- (1251234) // 800000)\n",
    "\n",
    "print(int(1251235/chunk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  business_name address_city address_state address_zip phone#    website_url\n",
      "0          BN 2         AC 2          AS 1        AZ 2   PH 2  www.jesse.com\n",
      "1          BN 5         AC 5          AS 1        AZ 5   PH 5    www.ccc.com\n",
      "2          BN 7         AC 7          AS 1        AZ 7   PH 7    www.eee.com\n",
      "4          BN 9         AC 9          AS 1        AZ 9   PH 9    www.ggg.com\n"
     ]
    }
   ],
   "source": [
    "data = [['business_name','address_city','address_state','address_zip','phone#','website_url'],\n",
    "['BN 2','AC 2','AS 1','AZ 2','PH 2','www.jesse.com'],\n",
    "['BN 5','AC 5','AS 1','AZ 5','PH 5','www.ccc.com'],\n",
    "['BN 7','AC 7','AS 1','AZ 7','PH 7','www.eee.com'],\n",
    "['BN 8','AC 8','AS 1','AZ 8','PH 8','www.JEsse.com'],\n",
    "['BN 9','AC 9','AS 1','AZ 9','PH 9','www.ggg.com']]\n",
    "headers = data.pop(0)\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(data, columns=headers)\n",
    "df['website_url'] = df['website_url'].str.lower() \n",
    "unique_data = df.drop_duplicates(subset = 'website_url')\n",
    "\n",
    "print(unique_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
